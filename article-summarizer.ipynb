{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: Tesfamariam\n",
    "gitHub: https://github.com/Tesfamariam100/module7-p7-article-summarizer/blob/main/article-summarizer.ipynb\n",
    "\n",
    "Date: Dec. 05 2024\n",
    "\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "annotated-types           0.7.0\n",
      "anyio                     4.7.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.4\n",
      "attrs                     24.2.0\n",
      "babel                     2.16.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.2.0\n",
      "blis                      1.0.1\n",
      "catalogue                 2.0.10\n",
      "certifi                   2024.8.30\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.0\n",
      "click                     8.1.7\n",
      "cloudpathlib              0.20.0\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "confection                0.1.5\n",
      "contourpy                 1.3.0\n",
      "cycler                    0.12.1\n",
      "cymem                     2.0.10\n",
      "debugpy                   1.8.9\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "exceptiongroup            1.2.2\n",
      "executing                 2.1.0\n",
      "fastjsonschema            2.21.1\n",
      "fonttools                 4.55.2\n",
      "fqdn                      1.5.1\n",
      "h11                       0.14.0\n",
      "html5lib                  1.1\n",
      "httpcore                  1.0.7\n",
      "httpx                     0.28.0\n",
      "idna                      3.10\n",
      "importlib_metadata        8.5.0\n",
      "importlib_resources       6.4.5\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.18.1\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.4\n",
      "joblib                    1.4.2\n",
      "json5                     0.10.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.2\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.3.2\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "kiwisolver                1.4.7\n",
      "langcodes                 3.5.0\n",
      "language_data             1.3.0\n",
      "marisa-trie               1.2.1\n",
      "markdown-it-py            3.0.0\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib                3.9.3\n",
      "matplotlib-inline         0.1.7\n",
      "mdurl                     0.1.2\n",
      "mistune                   3.0.2\n",
      "murmurhash                1.0.11\n",
      "nbclient                  0.10.1\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "nltk                      3.9.1\n",
      "notebook                  7.3.1\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.0.2\n",
      "overrides                 7.7.0\n",
      "packaging                 24.2\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pillow                    11.0.0\n",
      "pip                       21.2.4\n",
      "platformdirs              4.3.6\n",
      "preshed                   3.0.9\n",
      "prometheus_client         0.21.1\n",
      "prompt_toolkit            3.0.48\n",
      "psutil                    6.1.0\n",
      "pure_eval                 0.2.3\n",
      "pycparser                 2.22\n",
      "pydantic                  2.10.3\n",
      "pydantic_core             2.27.1\n",
      "Pygments                  2.18.0\n",
      "pyparsing                 3.2.0\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pywin32                   308\n",
      "pywinpty                  2.0.14\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.0\n",
      "referencing               0.35.1\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rich                      13.9.4\n",
      "rpds-py                   0.22.3\n",
      "Send2Trash                1.8.3\n",
      "setuptools                58.1.0\n",
      "shellingham               1.5.4\n",
      "six                       1.17.0\n",
      "smart-open                7.0.5\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "spacy                     3.8.2\n",
      "spacy-legacy              3.0.12\n",
      "spacy-loggers             1.0.5\n",
      "spacytextblob             5.0.0\n",
      "srsly                     2.4.8\n",
      "stack-data                0.6.3\n",
      "terminado                 0.18.1\n",
      "textblob                  0.18.0.post0\n",
      "thinc                     8.3.2\n",
      "tinycss2                  1.4.0\n",
      "tomli                     2.2.1\n",
      "tornado                   6.4.2\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "typer                     0.15.1\n",
      "types-python-dateutil     2.9.0.20241206\n",
      "typing_extensions         4.12.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.3\n",
      "wasabi                    1.1.3\n",
      "wcwidth                   0.2.13\n",
      "weasel                    0.4.1\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "wrapt                     1.17.0\n",
      "zipp                      3.21.0\n",
      "All prereqs installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Create and activate a Python virtual environment. \n",
    "# Before starting the project, try all these imports FIRST\n",
    "# Address any errors you get running this code cell \n",
    "# by installing the necessary packages into your active Python environment.\n",
    "# Try to resolve issues using your materials and the web.\n",
    "# If that doesn't work, ask for help in the discussion forums.\n",
    "# You can't complete the exercises until you import these - start early! \n",
    "# We also import pickle and Counter (included in the Python Standard Library).\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import requests\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import html5lib\n",
    "import matplotlib.pyplot as plt\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "!pip list\n",
    "\n",
    "print('All prereqs installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAO homepage HTML saved to fao_homepage.pkl\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import requests  # Library to make HTTP requests for websites\n",
    "import pickle    # Library used to save and load Python objects by serializing and deserializing\n",
    "\n",
    "# Request and Store Response\n",
    "fao_page = requests.get('https://www.fao.org/home/en/')\n",
    "fao_html = fao_page.text\n",
    "\n",
    "# Use Pickle library to serialize the fao_html string and store in file fao_homepage.pkl\n",
    "# The serialized file can later be retrieved and used without requesting from url\n",
    "with open('fao_homepage.pkl', 'wb') as file:\n",
    "    pickle.dump(fao_html, file)\n",
    "\n",
    "print('FAO homepage HTML saved to fao_homepage.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Text Extracted Successfully.\n",
      "Polarity score: 0.18\n",
      "The sentiment of the article is positive.\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\textblob\\decorators.py:35\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\textblob\\tokenizers.py:59\u001b[0m, in \u001b[0;36mSentenceTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of sentences.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03mA constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03ma lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m:type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1743\u001b[0m PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Administrator/nltk_data'\n    - 'c:\\\\Users\\\\Administrator\\\\OneDrive\\\\Documents\\\\CSIS-446Web\\\\module7-p7-article-summarizer\\\\m7p7_venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Administrator\\\\OneDrive\\\\Documents\\\\CSIS-446Web\\\\module7-p7-article-summarizer\\\\m7p7_venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Administrator\\\\OneDrive\\\\Documents\\\\CSIS-446Web\\\\module7-p7-article-summarizer\\\\m7p7_venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sentiment of the article is neutral.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Optionally, count the number of sentences in the article\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences\u001b[49m\n\u001b[0;32m     44\u001b[0m num_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentences)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of sentences in the article: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_sentences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\textblob\\decorators.py:23\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m---> 23\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\textblob\\blob.py:609\u001b[0m, in \u001b[0;36mTextBlob.sentences\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    608\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return list of :class:`Sentence <Sentence>` objects.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_sentence_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\textblob\\blob.py:652\u001b[0m, in \u001b[0;36mTextBlob._create_sentence_objects\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of Sentence objects from the raw text.\"\"\"\u001b[39;00m\n\u001b[0;32m    651\u001b[0m sentence_objects \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 652\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m char_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Keeps track of character index within the blob\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;66;03m# Compute the start and end indices of the sentence\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;66;03m# within the blob\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\textblob\\base.py:67\u001b[0m, in \u001b[0;36mBaseTokenizer.itokenize\u001b[1;34m(self, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a generator that generates tokens \"on-demand\".\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :rtype: generator\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Documents\\CSIS-446Web\\module7-p7-article-summarizer\\m7p7_venv\\lib\\site-packages\\textblob\\decorators.py:37\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingCorpusError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "# Load the HTML content from the pickle file\n",
    "pickle_file_path = 'fao_homepage.pkl'  # Adjust the file path if needed\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    fao_html = pickle.load(file)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(fao_html, 'html.parser')\n",
    "\n",
    "# Extract the text content of the article (or main content section)\n",
    "article_text = soup.find('article')  # You can adjust this if needed based on HTML structure\n",
    "if not article_text:\n",
    "    article_text = soup.find('main')  # Try 'main' tag if 'article' is not found\n",
    "\n",
    "# If no article tag found, extract all text from the page\n",
    "if article_text:\n",
    "    text = article_text.get_text(separator=\"\\n\", strip=True)\n",
    "else:\n",
    "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Check if text was extracted\n",
    "if text:\n",
    "    print(\"Article Text Extracted Successfully.\")\n",
    "\n",
    "    # Perform sentiment analysis using TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "\n",
    "    # Print the polarity score and sentiment label\n",
    "    print(f\"Polarity score: {polarity_score:.2f}\")\n",
    "    if polarity_score > 0:\n",
    "        print(\"The sentiment of the article is positive.\")\n",
    "    elif polarity_score < 0:\n",
    "        print(\"The sentiment of the article is negative.\")\n",
    "    else:\n",
    "        print(\"The sentiment of the article is neutral.\")\n",
    "\n",
    "    # Optionally, count the number of sentences in the article\n",
    "    sentences = blob.sentences\n",
    "    num_sentences = len(sentences)\n",
    "    print(f\"Number of sentences in the article: {num_sentences}\")\n",
    "else:\n",
    "    print(\"No article text found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most frequent tokens (excluding stopwords and punctuation):\n",
      "Token: '\n",
      "', Frequency: 171\n",
      "Token: 'fao', Frequency: 55\n",
      "Token: 'food', Frequency: 41\n",
      "Token: '\n",
      "                ', Frequency: 19\n",
      "Token: '\n",
      "                            ', Frequency: 18\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the small English model (en_core_web_sm) into spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with the spaCy pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize the text and filter out stopwords and punctuation\n",
    "tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Get the frequencies of tokens\n",
    "token_frequencies = Counter(tokens)\n",
    "\n",
    "# Get the 5 most common tokens\n",
    "common_tokens = token_frequencies.most_common(5)\n",
    "\n",
    "# Print the common tokens with their frequencies\n",
    "print(\"The 5 most frequent tokens (excluding stopwords and punctuation):\")\n",
    "for token, freq in common_tokens:\n",
    "    print(f\"Token: '{token}', Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most frequent lemmas (excluding stopwords and punctuation):\n",
      "Lemma: '\n",
      "', Frequency: 171\n",
      "Lemma: 'fao', Frequency: 55\n",
      "Lemma: 'food', Frequency: 44\n",
      "Lemma: '\n",
      "                ', Frequency: 19\n",
      "Lemma: 'work', Frequency: 18\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the small English model (en_core_web_sm) into spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with the spaCy pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract lemmas, convert to lowercase, and filter out stopwords and punctuation\n",
    "lemmas = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Get the frequencies of lemmas\n",
    "lemma_frequencies = Counter(lemmas)\n",
    "\n",
    "# Get the 5 most common lemmas\n",
    "common_lemmas = lemma_frequencies.most_common(5)\n",
    "\n",
    "# Print the common lemmas with their frequencies\n",
    "print(\"The 5 most frequent lemmas (excluding stopwords and punctuation):\")\n",
    "for lemma, freq in common_lemmas:\n",
    "    print(f\"Lemma: '{lemma}', Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Add SpacyTextBlob extension to spaCy\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m spacy_text_blob \u001b[38;5;241m=\u001b[39m \u001b[43mSpacyTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(spacy_text_blob, last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Process the text with spaCy\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'nlp'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# Load the small English model (en_core_web_sm) into spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add SpacyTextBlob extension to spaCy\n",
    "spacy_text_blob = SpacyTextBlob()\n",
    "nlp.add_pipe(spacy_text_blob, last=True)\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# List to store sentiment scores (polarity) of each sentence\n",
    "sentiment_scores = []\n",
    "\n",
    "# Loop through the sentences in the document\n",
    "for sent in doc.sents:\n",
    "    sentiment_scores.append(sent._.polarity)  # Get polarity score for each sentence\n",
    "\n",
    "# Plo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Add SpacyTextBlob extension to spaCy\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m spacy_text_blob \u001b[38;5;241m=\u001b[39m \u001b[43mSpacyTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(spacy_text_blob, last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Process the text with spaCy\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'nlp'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# Load the small English model (en_core_web_sm) into spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add SpacyTextBlob extension to spaCy\n",
    "spacy_text_blob = SpacyTextBlob()\n",
    "nlp.add_pipe(spacy_text_blob, last=True)\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# List to store sentiment scores (polarity) of each sentence using lemmas\n",
    "sentiment_scores_lemmas = []\n",
    "\n",
    "# Loop through the sentences in the document\n",
    "for sent in doc.sents:\n",
    "    # Create a list of lemmas for each token in the sentence\n",
    "    lemmas = [token.lemma_ for token in sent if token.is_alpha]  # Only include alphabetic tokens\n",
    "    # Calculate the sentiment score based on the lemmas\n",
    "    lemma_text = \" \".join(lemmas)\n",
    "    \n",
    "    # Process the lemma text and calculate its polarity score\n",
    "    lemma_doc = nlp(lemma_text)\n",
    "    sentiment_scores_lemmas.append(lemma_doc._.polarity)  # Get polarity score for the lemma-based sentence\n",
    "\n",
    "# Plot the histogram of the sentiment scores based on lemmas\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(sentiment_scores_lemmas, bins=20, edgecolor='black')\n",
    "plt.title('Sentiment Score Distribution per Sentence (Lemmas)')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Comment on the most common range of sentiment scores based on the histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_scores_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Calculate the median score for tokens\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m cutoff_score_tokens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmedian(\u001b[43msentiment_scores_tokens\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate the median score for lemmas\u001b[39;00m\n\u001b[0;32m      7\u001b[0m cutoff_score_lemmas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmedian(sentiment_scores_lemmas)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentiment_scores_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the median score for tokens\n",
    "cutoff_score_tokens = np.median(sentiment_scores_tokens)\n",
    "\n",
    "# Calculate the median score for lemmas\n",
    "cutoff_score_lemmas = np.median(sentiment_scores_lemmas)\n",
    "\n",
    "# Print the cutoff scores\n",
    "print(f\"Cutoff Score (tokens): {cutoff_score_tokens:.2f}\")\n",
    "print(f\"Cutoff Score (lemmas): {cutoff_score_lemmas:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_scores_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Calculate the median score for tokens\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m cutoff_score_tokens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmedian(\u001b[43msentiment_scores_tokens\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate the median score for lemmas\u001b[39;00m\n\u001b[0;32m      7\u001b[0m cutoff_score_lemmas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmedian(sentiment_scores_lemmas)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentiment_scores_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the median score for tokens\n",
    "cutoff_score_tokens = np.median(sentiment_scores_tokens)\n",
    "\n",
    "# Calculate the median score for lemmas\n",
    "cutoff_score_lemmas = np.median(sentiment_scores_lemmas)\n",
    "\n",
    "# Print the cutoff scores\n",
    "print(f\"Cutoff Score (tokens): {cutoff_score_tokens:.2f}\")\n",
    "print(f\"Cutoff Score (lemmas): {cutoff_score_lemmas:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Convert the summary text into a TextBlob object\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m summary_blob \u001b[38;5;241m=\u001b[39m TextBlob(\u001b[43msummary_text\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate the polarity score of the summary\u001b[39;00m\n\u001b[0;32m      7\u001b[0m polarity_score \u001b[38;5;241m=\u001b[39m summary_blob\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39mpolarity\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summary_text' is not defined"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Convert the summary text into a TextBlob object\n",
    "summary_blob = TextBlob(summary_text)\n",
    "\n",
    "# Calculate the polarity score of the summary\n",
    "polarity_score = summary_blob.sentiment.polarity\n",
    "\n",
    "# Count the number of sentences in the summary\n",
    "num_sentences = len(summary_sentences)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Polarity score of the summary: {polarity_score:.2f}\")\n",
    "print(f\"Number of sentences in the summarized article: {num_sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'module' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Iterate over every sentence in the article\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Calculate the score based on lemmas\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     lemma_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([token\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sent \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m spacy\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39men\u001b[38;5;241m.\u001b[39mstop_words])\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Add the sentence to the summary if its score is greater than the cutoff\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lemma_score \u001b[38;5;241m>\u001b[39m cutoff_score_lemmas:\n",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Iterate over every sentence in the article\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Calculate the score based on lemmas\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     lemma_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([token\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sent \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43men\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop_words\u001b[49m])\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Add the sentence to the summary if its score is greater than the cutoff\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lemma_score \u001b[38;5;241m>\u001b[39m cutoff_score_lemmas:\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'module' is not iterable"
     ]
    }
   ],
   "source": [
    "# List to hold the sentences that are part of the summary based on lemmas\n",
    "lemma_summary_sentences = []\n",
    "\n",
    "# Iterate over every sentence in the article\n",
    "for sent in doc.sents:\n",
    "    # Calculate the score based on lemmas\n",
    "    lemma_score = sum([token.rank for token in sent if token.lemma_ not in spacy.lang.en.stop_words])\n",
    "    \n",
    "    # Add the sentence to the summary if its score is greater than the cutoff\n",
    "    if lemma_score > cutoff_score_lemmas:\n",
    "        lemma_summary_sentences.append(sent.text.strip())\n",
    "\n",
    "# Generate the summary text by joining the sentences\n",
    "lemma_summary_text = ' '.join(lemma_summary_sentences)\n",
    "\n",
    "# Print the summary\n",
    "print(lemma_summary_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemma_summary_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Generate a TextBlob object for the summary\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m summary_blob \u001b[38;5;241m=\u001b[39m TextBlob(\u001b[43mlemma_summary_text\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate the polarity score of the summary\u001b[39;00m\n\u001b[0;32m      7\u001b[0m polarity_score_lemma_summary \u001b[38;5;241m=\u001b[39m summary_blob\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39mpolarity\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemma_summary_text' is not defined"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Generate a TextBlob object for the summary\n",
    "summary_blob = TextBlob(lemma_summary_text)\n",
    "\n",
    "# Calculate the polarity score of the summary\n",
    "polarity_score_lemma_summary = summary_blob.sentiment.polarity\n",
    "\n",
    "# Print the polarity score and the number of sentences in the summary\n",
    "print(f\"Polarity score of the summary (based on lemmas): {polarity_score_lemma_summary:.2f}\")\n",
    "print(f\"Number of sentences in the summary: {len(lemma_summary_sentences)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m7p7_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
